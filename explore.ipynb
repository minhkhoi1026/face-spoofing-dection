{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtle/miniconda3/envs/fsd/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-09 18:46:48.700740: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-09 18:46:48.877347: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-09 18:46:49.597504: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dtle/miniconda3/envs/fsd/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda/lib\n",
      "2023-09-09 18:46:49.597570: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dtle/miniconda3/envs/fsd/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda/lib\n",
      "2023-09-09 18:46:49.597576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "from src.utils.opt import Config\n",
    "from src.model import MODEL_REGISTRY\n",
    "from demo.extractor.frame_extractor import FrameExtractor\n",
    "from demo.extractor.face_fafi_extractor import FaceFAFIExtractor\n",
    "from demo.utils.gradcam import GradCAMCompatibleModel\n",
    "from demo.detector.lightning_detector import TorchLightningDetector\n",
    "\n",
    "class MobileNetDetector(TorchLightningDetector):\n",
    "    def __init__(self,\n",
    "                 name, \n",
    "                 cfg_path: str=\"configs/inference/double_head_mobilenet_fafi_hybrid.yml\",\n",
    "                 frame_width=1280,\n",
    "                 frame_height=720,\n",
    "                 thickness_percentage=10, \n",
    "                 blur_percentage=10,\n",
    "                 *args,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        super().__init__(name,\n",
    "                         cfg_path, \n",
    "                         frame_width, \n",
    "                         frame_height, \n",
    "                         thickness_percentage, \n",
    "                         blur_percentage, \n",
    "                         *args,\n",
    "                         **kwargs\n",
    "                         )\n",
    "\n",
    "    def create_gradcam_model(self, cfg):\n",
    "        grad_cam_model = GradCAMCompatibleModel(self.create_detector_model(cfg))\n",
    "        cam = GradCAMPlusPlus(model=grad_cam_model,\n",
    "                            target_layers=[\n",
    "                                grad_cam_model.model.img_extractor.extractor.blocks[-1],\n",
    "                                grad_cam_model.model.img_variant_extractor.extractor.blocks[-1]\n",
    "                                ],\n",
    "                            use_cuda=torch.cuda.is_available(),\n",
    "                            )\n",
    "        cam.batch_size = cfg['data_loader']['test']['args']['batch_size']\n",
    "        return grad_cam_model, cam\n",
    "    \n",
    "    def get_grad_cam(self, batch):\n",
    "        n = batch[\"imgs\"].shape[0]\n",
    "        targets = [ClassifierOutputTarget(1) for i in range(n)]\n",
    "        return self.cam(input_tensor=torch.cat((batch[\"imgs\"], batch[\"img_variants\"]), dim=1),\n",
    "                                            targets=targets,\n",
    "                                            eigen_smooth=False,\n",
    "                                            aug_smooth=False)\n",
    "    \n",
    "\n",
    "import argparse\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "features = ['msr', 'fafi']\n",
    "detector = {}\n",
    "detector[\"msr\"] = MobileNetDetector(\"mobilenet\", \"configs/inference/double_head_mobilenet_{}_hybrid.yml\".format(\"msr\"))\n",
    "detector[\"fafi\"] = MobileNetDetector(\"mobilenet\", \"configs/inference/double_head_mobilenet_{}_hybrid.yml\".format(\"fafi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset_v1/videos/roop/fake/200_ntthau.mp4', 'dataset_v1/videos/roop/fake/067.mp4', 'dataset_v1/videos/roop/fake/201_ntthau.mp4', 'dataset_v1/videos/roop/fake/068_nqktuyen.mp4', 'dataset_v1/videos/roop/fake/066_nqktuyen.mp4', 'dataset_v1/videos/deepfaker/fake/video008_ttha.mp4', 'dataset_v1/videos/deepfaker/fake/video022.mp4', 'dataset_v1/videos/roop/fake/030.mp4', 'dataset_v1/videos/roop/fake/034_sedanh.mp4', 'dataset_v1/videos/roop/fake/034.mp4', 'dataset_v1/videos/roop/fake/067_nqktuyen.mp4', 'dataset_v1/videos/deepfaker/fake/video008.mp4', 'dataset_v1/videos/deepfaker/fake/video104_nqduy.mp4', 'dataset_v1/videos/roop/fake/201.mp4', 'dataset_v1/videos/deepfaker/fake/video104.mp4', 'dataset_v1/videos/roop/fake/074.mp4', 'dataset_v1/videos/deepfaker/fake/video105.mp4', 'dataset_v1/videos/deepfaker/fake/video102.mp4', 'dataset_v1/videos/deepfaker/fake/video103_nqduy.mp4', 'dataset_v1/videos/roop/fake/202.mp4', 'dataset_v1/videos/roop/fake/069.mp4', 'dataset_v1/videos/roop/fake/204.mp4', 'dataset_v1/videos/roop/fake/071.mp4', 'dataset_v1/videos/roop/fake/203_ntthau.mp4', 'dataset_v1/videos/roop/fake/069_nqktuyen.mp4', 'dataset_v1/videos/deepfaker/fake/video007_ttha.mp4', 'dataset_v1/videos/deepfaker/fake/video009_ttha.mp4', 'dataset_v1/videos/roop/fake/031.mp4', 'dataset_v1/videos/roop/fake/033.mp4', 'dataset_v1/videos/roop/fake/203.mp4', 'dataset_v1/videos/deepfaker/fake/video103.mp4', 'dataset_v1/videos/roop/fake/200.mp4', 'dataset_v1/videos/roop/fake/072.mp4', 'dataset_v1/videos/roop/fake/074_mnhduong.mp4', 'dataset_v1/videos/roop/fake/068.mp4', 'dataset_v1/videos/roop/fake/070.mp4', 'dataset_v1/videos/roop/fake/070_mnhduong.mp4', 'dataset_v1/videos/roop/fake/071_mnhduong.mp4', 'dataset_v1/videos/deepfaker/fake/video007.mp4', 'dataset_v1/videos/roop/fake/032_sedanh.mp4', 'dataset_v1/videos/roop/fake/202_ntthau.mp4', 'dataset_v1/videos/deepfaker/fake/video009.mp4', 'dataset_v1/videos/roop/fake/073_mnhduong.mp4', 'dataset_v1/videos/roop/fake/032.mp4', 'dataset_v1/videos/deepfaker/fake/video023.mp4', 'dataset_v1/videos/deepfaker/fake/video105_nqduy.mp4', 'dataset_v1/videos/roop/fake/072_mnhduong.mp4', 'dataset_v1/videos/deepfaker/fake/video023_ttha.mp4', 'dataset_v1/videos/roop/fake/073.mp4', 'dataset_v1/videos/roop/fake/065_nqktuyen.mp4', 'dataset_v1/videos/roop/fake/030_sedanh.mp4', 'dataset_v1/videos/roop/fake/031_sedanh.mp4', 'dataset_v1/videos/roop/fake/204_ntthau.mp4', 'dataset_v1/videos/roop/fake/065.mp4', 'dataset_v1/videos/roop/fake/066.mp4', 'dataset_v1/videos/deepfaker/fake/video102_nqduy.mp4', 'dataset_v1/videos/deepfaker/fake/video022_ttha.mp4']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset_v1/imgs/val.csv\")\n",
    "video_list = df[\"filepath\"].apply(lambda x: os.path.splitext(os.path.basename(x))[0]).to_list()\n",
    "video_list = [os.path.join(\"dataset_v1/videos\",\"deepfaker\" if x.startswith(\"video\") else \"roop\", \"fake\", '_'.join(x.split(\"_\")[:-1])+\".mp4\") for x in video_list]\n",
    "video_list = list(set(video_list))\n",
    "print(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 719/719 [00:00<00:00, 983.39it/s] \n",
      "/home/dtle/hnttruc-local/face-spoofing-dection-ABFTSCNN/demo/detector/lightning_detector.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = F.softmax(logits)[:, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 719/719 [00:00<00:00, 1029.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224)\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "video_list = [\"dataset_v1/videos/roop/fake/200_ntthau.mp4\"]\n",
    "for video in video_list:\n",
    "    if not os.path.exists(video): continue\n",
    "    video_basename = os.path.splitext(os.path.basename(video))[0]\n",
    "    for feature in features:\n",
    "        x = detector[feature].predict(open(video, \"rb\").read(), sampling=10000)\n",
    "\n",
    "        try:\n",
    "            image_path= x.iloc[0][\"predict\"][0][\"face_path\"]\n",
    "            shutil.copyfile(image_path, f\"imgs/detector_cam_mobilenet_{video_basename}_source{os.path.splitext(image_path)[1]}\")\n",
    "            grayscale_cam = x.iloc[0][\"predict\"][0][\"grad_cam\"]\n",
    "            print(grayscale_cam.shape)\n",
    "            rgb_img = cv2.imread(image_path, 1)[:, :, ::-1]\n",
    "            rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "            rgb_img = np.float32(rgb_img) / 255\n",
    "\n",
    "            cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n",
    "            cv2.imwrite(f'imgs/detector_cam_mobilenet_{video_basename}_{feature}.jpg', cam_image)\n",
    "        except Exception as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
