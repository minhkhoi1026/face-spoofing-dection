{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given one input image:\n",
    "1. Use MTCNN for face and landmark detection\n",
    "2. Align detected faces using affine transformation to size $128 \\times 128$\n",
    "3. If in training mode, apply data augmentation including horizontal flipping, random rotation ($0$-$20$ degree), and random crop ($114 \\times 114$).\n",
    "4. Generate MSR (multiscale retinex) image from RGB image\n",
    "\n",
    "Training params:\n",
    "- Momentum: $0.9$\n",
    "- Learning rate: $10^{-4}$\n",
    "- Batch size: $128$\n",
    "- Number of epoch to converge: $50$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"source: https://github.com/ondyari/FaceForensics + https://github.com/ipazc/mtcnn\"\"\"\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from mtcnn import MTCNN\n",
    "import tensorflow as tf\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "def extract_faces(image, output_path, prefix):\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    with tf.device('/GPU:0'):\n",
    "        faces = detector.detect_faces(img)\n",
    "    crop_face = 0\n",
    "    for id, face in enumerate(faces):\n",
    "        x, y, w, h = face['box']\n",
    "        crop_face = cv2.cvtColor(img[y:y+h, x:x+w], cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(os.path.join(output_path, '{}_{:02d}.png'.format(prefix, id)), crop_face)\n",
    "\n",
    "def extract_frames(data_path, output_path, prefix_images, method='cv2'):\n",
    "    \"\"\"Method to extract frames, either with ffmpeg or opencv.\"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    reader = cv2.VideoCapture(data_path)\n",
    "    frame_num = 0\n",
    "    while reader.isOpened():\n",
    "        success, image = reader.read()\n",
    "        if not success:\n",
    "            break\n",
    "        prefix_face_img = '{}_{:04d}'.format(prefix_images, frame_num)\n",
    "        extract_faces(image, output_path, prefix_face_img) # extract faces from single image\n",
    "        frame_num += 1\n",
    "    reader.release()\n",
    "    \n",
    "def extract_individual(individual_path, output_path, individual_name):\n",
    "    \"\"\"Extracts all videos file structure\"\"\"\n",
    "    for video in os.listdir(individual_path):\n",
    "        # prefix of image file name\n",
    "        video_name = os.path.splitext(video)[0]\n",
    "        prefix = f\"{individual_name}_{video_name}\"\n",
    "        \n",
    "        # folder for store image base on type of image \n",
    "        # image have name 1, 2 or HR_1 are real, others are fake\n",
    "        image_type = \"real\" if video_name in [\"1\", \"2\", \"HR_1\"] else \"fake\"\n",
    "        print(video_name)\n",
    "        image_path = os.path.join(output_path, image_type)\n",
    "        \n",
    "        extract_frames(os.path.join(individual_path, video),\n",
    "                       image_path, prefix)\n",
    "\n",
    "def extract_all_individual(data_path, output_path):\n",
    "    for individual in tqdm(os.listdir(data_path)):\n",
    "        extract_individual(os.path.join(data_path, individual), output_path, individual)\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "extract_all_individual(os.path.join(\"dataset\", \"casia_fasd\", \"test_release\"), \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c369725eaf0bcedc0e855cf18b5143763a0b6300dfb6d5d06050fb124ccb31b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
